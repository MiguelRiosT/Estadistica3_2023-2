{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9405a7b-7a95-4a0f-9b19-93a557dcbba5",
   "metadata": {},
   "source": [
    "# Tarea 4 - Miguel Rios Tangarife - Punto 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa40531-3a5a-46ce-a762-ecf5eb191208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Miguel\n",
      "[nltk_data]     Rios\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import tarfile\n",
    "import zipfile\n",
    "from  sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.io import loadmat\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.datasets import imdb\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee3623-ba3b-4317-8981-fadce577d147",
   "metadata": {},
   "source": [
    "## Regresión logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236d9c17-6141-453f-aba4-6ae6dd8d571c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: Regresión logística\n",
      "Entrenamiento: 0.856, Testeo: 0.637\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "dataTrain = pd.read_csv('data/train/trainingpunto2.csv')\n",
    "dataTest = pd.read_csv('data/test/testpunto2.csv')\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X = dataTrain[\"titulo\"]\n",
    "y = dataTrain[\"categoria\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Inicializar el vectorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Aplicar el vectorizador a los datos de entrenamiento y prueba\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Inicializar y entrenar el modelo de regresión logística\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', max_iter=100)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Calcular la precisión en el conjunto de entrenamiento y prueba\n",
    "train_acc = clf.score(X_train_tfidf, y_train)\n",
    "test_acc = clf.score(X_test_tfidf, y_test)\n",
    "print('Usando: Regresión logística')\n",
    "print('Entrenamiento: %.3f, Testeo: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6d8cb-482a-4c1b-940c-91d9cbefff52",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9205481-1434-400d-93cc-d54f493635f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando: Random forest\n",
      "Entrenamiento: 1.000, Testeo: 0.650\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "dataTrain = pd.read_csv('data/train/trainingpunto2.csv')\n",
    "dataTest = pd.read_csv('data/test/testpunto2.csv')\n",
    "    \n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X = dataTrain[\"titulo\"]\n",
    "y = dataTrain[\"categoria\"]\n",
    "    \n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "# Inicializar el vectorizador TF-IDF con eliminación de stop words en español\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "# Aplicar el vectorizador a los datos de entrenamiento y prueba\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "# Inicializar y entrenar el modelo Random Forest\n",
    "rf_classifier = RandomForestClassifier(random_state=0, n_estimators=500)\n",
    "rf_classifier.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "# Calcular la precisión en el conjunto de entrenamiento y prueba\n",
    "train_acc = rf_classifier.score(X_train_tfidf, y_train)\n",
    "test_acc = rf_classifier.score(X_test_tfidf, y_test)\n",
    "print('Usando: Random forest')\n",
    "print('Entrenamiento: %.3f, Testeo: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d502b9-2dbc-4433-8607-9667db6bff52",
   "metadata": {},
   "source": [
    "## LSTM (Long short-term memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582543f2-900e-44f3-9edc-8ee3b9982ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10/10 [==============================] - 2s 55ms/step - loss: 1.7649 - accuracy: 0.2906\n",
      "Epoch 2/15\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 1.6601 - accuracy: 0.2781\n",
      "Epoch 3/15\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 1.6137 - accuracy: 0.3938\n",
      "Epoch 4/15\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 1.5492 - accuracy: 0.5141\n",
      "Epoch 5/15\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 1.4268 - accuracy: 0.5797\n",
      "Epoch 6/15\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 1.3309 - accuracy: 0.5328\n",
      "Epoch 7/15\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 1.1875 - accuracy: 0.6516\n",
      "Epoch 8/15\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 1.0224 - accuracy: 0.6969\n",
      "Epoch 9/15\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.8663 - accuracy: 0.7375\n",
      "Epoch 10/15\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.7361 - accuracy: 0.7750\n",
      "Epoch 11/15\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 0.6164 - accuracy: 0.8141\n",
      "Epoch 12/15\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.5164 - accuracy: 0.8313\n",
      "Epoch 13/15\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.4419 - accuracy: 0.8609\n",
      "Epoch 14/15\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.3682 - accuracy: 0.8813\n",
      "Epoch 15/15\n",
      "10/10 [==============================] - 1s 53ms/step - loss: 0.4010 - accuracy: 0.9094\n",
      "Usando: LSTM\n",
      "Entrenamiento: 0.944, Testeo: 0.613\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos\n",
    "dataTrain = pd.read_csv('data/train/trainingpunto2.csv')\n",
    "dataTest = pd.read_csv('data/test/testpunto2.csv')\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X = dataTrain[\"titulo\"]\n",
    "y = dataTrain[\"categoria\"]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar el tokenizador y ajustar el vocabulario\n",
    "max_words = 25000  # Número máximo de palabras en el vocabulario\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convertir las secuencias de palabras a secuencias de números\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad secuencias para que tengan la misma longitud\n",
    "max_sequence_length = 100  # Longitud máxima de secuencia\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Inicializar un codificador de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Codificar las etiquetas\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Obtener el número de clases después de la codificación\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Crear un modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 64, input_length=max_sequence_length))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Capa de salida para clasificación múltiple\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# One-hot encode las etiquetas\n",
    "y_train_encoded = to_categorical(y_train_encoded, num_classes)\n",
    "y_test_encoded = to_categorical(y_test_encoded, num_classes)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train_padded, y_train_encoded, epochs=15, batch_size=64)\n",
    "\n",
    "# Calcular la precisión en el conjunto de entrenamiento y prueba\n",
    "train_acc = model.evaluate(X_train_padded, y_train_encoded, verbose=0)[1]\n",
    "test_acc = model.evaluate(X_test_padded, y_test_encoded, verbose=0)[1]\n",
    "print('Usando: LSTM')\n",
    "print('Entrenamiento: %.3f, Testeo: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ba2df7-cb46-4095-9fd2-51b338655fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtener los datos de prueba\n",
    "X_test = dataTest[\"titulo\"]\n",
    "\n",
    "# Tokenizar y pad las secuencias de prueba\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Realizar predicciones en los datos de prueba\n",
    "predictions = model.predict(X_test_padded)\n",
    "\n",
    "# Obtener las categorías predichas para cada instancia\n",
    "predicted_categories = [label_encoder.classes_[i] for i in predictions.argmax(axis=1)]\n",
    "\n",
    "# Crear un DataFrame con 'id' y 'categoria' para la sumisión\n",
    "submission_df = dataTest[['index']]\n",
    "submission_df['categoria'] = predicted_categories\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "submission_df.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e224c-6791-4c54-a5e3-6563c81f8d75",
   "metadata": {},
   "source": [
    "## Matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8da434-358f-42e1-a152-e0ae4733ca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión para Regresión Logística:\n",
      "[[34  0  2  1  0  1]\n",
      " [ 4  6  6  0  0  8]\n",
      " [ 3  0 38  0  0  7]\n",
      " [ 1  0  5  1  0  1]\n",
      " [ 2  0  3  0  4  1]\n",
      " [ 4  2  7  0  0 19]]\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            Deportes       0.71      0.89      0.79        38\n",
      "           Educación       0.75      0.25      0.38        24\n",
      "     Entretenimiento       0.62      0.79      0.70        48\n",
      "       Gente y Blogs       0.50      0.12      0.20         8\n",
      "               Otros       1.00      0.40      0.57        10\n",
      "Película y Animación       0.51      0.59      0.55        32\n",
      "\n",
      "            accuracy                           0.64       160\n",
      "           macro avg       0.68      0.51      0.53       160\n",
      "        weighted avg       0.66      0.64      0.61       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = clf.predict(X_test_tfidf)\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"Matriz de Confusión para Regresión Logística:\")\n",
    "print(cm_lr)\n",
    "report_lr = classification_report(y_test, y_pred_lr)\n",
    "print(report_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06e7d94-ad38-4fc8-9372-16ad4a5f2b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión para Random Forest:\n",
      "[[31  2  3  1  0  1]\n",
      " [ 1 13  6  0  2  2]\n",
      " [ 2  2 37  0  0  7]\n",
      " [ 1  0  4  3  0  0]\n",
      " [ 2  0  3  0  4  1]\n",
      " [ 4  6  6  0  0 16]]\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            Deportes       0.76      0.82      0.78        38\n",
      "           Educación       0.57      0.54      0.55        24\n",
      "     Entretenimiento       0.63      0.77      0.69        48\n",
      "       Gente y Blogs       0.75      0.38      0.50         8\n",
      "               Otros       0.67      0.40      0.50        10\n",
      "Película y Animación       0.59      0.50      0.54        32\n",
      "\n",
      "            accuracy                           0.65       160\n",
      "           macro avg       0.66      0.57      0.60       160\n",
      "        weighted avg       0.65      0.65      0.64       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf = rf_classifier.predict(X_test_tfidf)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"Matriz de Confusión para Random Forest:\")\n",
    "print(cm_rf)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "print(report_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
